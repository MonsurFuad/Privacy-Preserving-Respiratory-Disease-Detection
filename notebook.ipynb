{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFNl1nFxKbUX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "import os\n",
        "import copy\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import secrets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9bCynZfKbXY"
      },
      "outputs": [],
      "source": [
        "# Paths to the dataset directories\n",
        "train_dir = '/content/drive/MyDrive/Trial Dataset/train'\n",
        "test_dir  = '/content/drive/MyDrive/Trial Dataset/test'\n",
        "val_dir   = '/content/drive/MyDrive/Trial Dataset/validation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxdmfGXrKbZ_"
      },
      "outputs": [],
      "source": [
        "# Define the CNN Model\n",
        "class XRayClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(XRayClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1 channel for grayscale\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 56 * 56, 128)  # Assuming 224x224 input\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3ahZrUaKbcy"
      },
      "outputs": [],
      "source": [
        "# Load dataset from provided directories\n",
        "def get_dataset():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "        val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "        test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "        if len(train_dataset) == 0:\n",
        "            raise ValueError(f\"Training dataset is empty in {train_dir}\")\n",
        "        if len(val_dataset) == 0:\n",
        "            raise ValueError(f\"Validation dataset is empty in {val_dir}\")\n",
        "        if len(test_dataset) == 0:\n",
        "            raise ValueError(f\"Test dataset is empty in {test_dir}\")\n",
        "\n",
        "        num_classes = len(train_dataset.classes)\n",
        "        print(f\"Number of classes detected: {num_classes}\")\n",
        "        print(f\"Class names: {train_dataset.classes}\")\n",
        "\n",
        "        train_labels = [label for _, label in train_dataset.samples]\n",
        "        unique_labels = np.unique(train_labels)\n",
        "        label_counts = np.bincount(train_labels)\n",
        "        print(f\"Unique labels in training dataset: {unique_labels}\")\n",
        "        print(f\"Label distribution in training dataset:\")\n",
        "        for i, count in enumerate(label_counts):\n",
        "            if count > 0:\n",
        "                print(f\"  Class {i} ({train_dataset.classes[i]}): {count} samples\")\n",
        "        if max(unique_labels) >= num_classes:\n",
        "            raise ValueError(f\"Labels found ({unique_labels}) exceed number of classes ({num_classes})\")\n",
        "\n",
        "        print(f\"Loaded {len(train_dataset)} training samples, {len(val_dataset)} validation samples, {len(test_dataset)} test samples\")\n",
        "        return train_dataset, val_dataset, test_dataset, num_classes\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading datasets: {e}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSurq8KlKbff"
      },
      "outputs": [],
      "source": [
        "# Split dataset into non-IID for clients (hospitals)\n",
        "def split_dataset(dataset, num_clients=3):\n",
        "    client_datasets = []\n",
        "    data_len = len(dataset)\n",
        "    if data_len < num_clients:\n",
        "        raise ValueError(f\"Dataset size ({data_len}) is smaller than number of clients ({num_clients})\")\n",
        "\n",
        "    indices = list(range(data_len))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_sizes = [max(data_len // num_clients, 1)] * num_clients\n",
        "    remaining = data_len - sum(split_sizes)\n",
        "    for i in range(remaining):\n",
        "        split_sizes[i] += 1\n",
        "\n",
        "    split_indices = np.split(indices[:sum(split_sizes)], np.cumsum(split_sizes)[:-1])\n",
        "\n",
        "    for i, client_idx in enumerate(split_indices):\n",
        "        if len(client_idx) == 0:\n",
        "            raise ValueError(f\"Client {i+1} received an empty dataset\")\n",
        "        client_datasets.append(Subset(dataset, client_idx))\n",
        "        print(f\"Client {i+1} assigned {len(client_idx)} samples\")\n",
        "\n",
        "    return client_datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUQ3PjdyKmZI"
      },
      "outputs": [],
      "source": [
        "# Add Differential Privacy with Adaptive Noise\n",
        "def add_dp_noise(model, clip_norm=1.0, noise_multiplier=2.5, batch_size=32):\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norm = param.grad.norm()\n",
        "            if grad_norm > clip_norm:\n",
        "                param.grad *= clip_norm / grad_norm\n",
        "            noise = torch.randn_like(param.grad) * (noise_multiplier * clip_norm / batch_size)\n",
        "            param.grad += noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE97VRF7KmcO"
      },
      "outputs": [],
      "source": [
        "# Secure Aggregation: Generate and apply additive masks\n",
        "def secure_aggregate(client_models, num_clients):\n",
        "    # Simulate pairwise additive masks\n",
        "    masks = [{} for _ in range(num_clients)]\n",
        "    for key in client_models[0].state_dict().keys():\n",
        "        param_shape = client_models[0].state_dict()[key].shape\n",
        "        # Generate random masks for each client\n",
        "        for i in range(num_clients):\n",
        "            masks[i][key] = torch.randn(param_shape) * 100.0  # Large random mask\n",
        "        # Ensure masks sum to zero across clients\n",
        "        sum_masks = sum(m[key] for m in masks)\n",
        "        for i in range(num_clients):\n",
        "            masks[i][key] -= sum_masks / num_clients\n",
        "\n",
        "    # Apply masks to client updates\n",
        "    masked_updates = []\n",
        "    for i, model in enumerate(client_models):\n",
        "        masked_state = copy.deepcopy(model.state_dict())\n",
        "        for key in masked_state.keys():\n",
        "            masked_state[key] += masks[i][key]\n",
        "        masked_updates.append(masked_state)\n",
        "\n",
        "    # Aggregate masked updates\n",
        "    global_dict = copy.deepcopy(masked_updates[0])\n",
        "    for key in global_dict.keys():\n",
        "        global_dict[key] = torch.stack([update[key].float() for update in masked_updates], 0).mean(0)\n",
        "\n",
        "    return global_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sJ9Lq3NKmfA"
      },
      "outputs": [],
      "source": [
        "# Local training on a client with adaptive DP\n",
        "def local_train(model, dataloader, epochs=5, lr=0.001, device='cpu', dataset_size=1):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Adaptive DP: Increase noise for smaller datasets\n",
        "    noise_multiplier = 1.0 if dataset_size >= 100 else 1.0 + (100 - dataset_size) / 100.0\n",
        "    print(f\"Dataset size: {dataset_size}, Noise multiplier: {noise_multiplier:.4f}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply DP\n",
        "            add_dp_noise(model, clip_norm=1.0, noise_multiplier=noise_multiplier, batch_size=inputs.size(0))\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqpGwFTqKmho"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on validation/test set\n",
        "def evaluate_model(model, dataloader, device='cpu'):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else float('inf')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'loss': avg_loss\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK0Ai_RkKmki"
      },
      "outputs": [],
      "source": [
        "# Main Federated Learning Loop with Secure Aggregation\n",
        "def federated_learning(num_clients=3, global_epochs=10, local_epochs=5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Get and split dataset\n",
        "    try:\n",
        "        train_dataset, val_dataset, test_dataset, num_classes = get_dataset()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load datasets: {e}\")\n",
        "        return\n",
        "\n",
        "    global_model = XRayClassifier(num_classes=num_classes).to(device)\n",
        "\n",
        "    try:\n",
        "        client_datasets = split_dataset(train_dataset, num_clients)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to split dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    for round in range(global_epochs):\n",
        "        print(f\"Global Round {round + 1}/{global_epochs}\")\n",
        "        client_models = []\n",
        "\n",
        "        for client_id in range(num_clients):\n",
        "            print(f\"Training on Client {client_id + 1}\")\n",
        "            local_model = copy.deepcopy(global_model)\n",
        "            dataloader = DataLoader(client_datasets[client_id], batch_size=32, shuffle=True)\n",
        "            local_model = local_train(\n",
        "                local_model,\n",
        "                dataloader,\n",
        "                epochs=local_epochs,\n",
        "                device=device,\n",
        "                dataset_size=len(client_datasets[client_id])\n",
        "            )\n",
        "            client_models.append(local_model)\n",
        "\n",
        "        # Secure aggregation\n",
        "        global_dict = secure_aggregate(client_models, num_clients)\n",
        "        global_model.load_state_dict(global_dict)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_metrics = evaluate_model(global_model, val_loader, device=device)\n",
        "        print(f\"Validation Metrics - Round {round + 1}:\")\n",
        "        print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
        "        print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
        "        print(f\"F1 Score: {val_metrics['f1']:.4f}\")\n",
        "        print(f\"Loss: {val_metrics['loss']:.4f}\")\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(val_metrics['confusion_matrix'])\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    test_metrics = evaluate_model(global_model, test_loader, device=device)\n",
        "    print(\"\\nFinal Test Metrics:\")\n",
        "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
        "    print(f\"Loss: {test_metrics['loss']:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(test_metrics['confusion_matrix'])\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save(global_model.state_dict(), 'fl_dp_sa_recommandation.pth')\n",
        "    print(\"Training complete. Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERzCRHQbLX3P",
        "outputId": "7e81c001-57ae-4dee-e237-3c22dc6429dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes detected: 4\n",
            "Class names: ['Covid', 'Normal', 'Pneumonia', 'TB']\n",
            "Unique labels in training dataset: [0 1 2 3]\n",
            "Label distribution in training dataset:\n",
            "  Class 0 (Covid): 850 samples\n",
            "  Class 1 (Normal): 800 samples\n",
            "  Class 2 (Pneumonia): 800 samples\n",
            "  Class 3 (TB): 560 samples\n",
            "Loaded 3010 training samples, 370 validation samples, 370 test samples\n",
            "Client 1 assigned 1004 samples\n",
            "Client 2 assigned 1003 samples\n",
            "Client 3 assigned 1003 samples\n",
            "Global Round 1/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 1:\n",
            "Accuracy: 0.8486\n",
            "Precision: 0.8528\n",
            "Recall: 0.8486\n",
            "F1 Score: 0.8476\n",
            "Loss: 0.4619\n",
            "Confusion Matrix:\n",
            "[[91  1  4  4]\n",
            " [ 2 93  5  0]\n",
            " [ 1 16 79  4]\n",
            " [ 5  9  5 51]]\n",
            "Global Round 2/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 2:\n",
            "Accuracy: 0.8541\n",
            "Precision: 0.8643\n",
            "Recall: 0.8541\n",
            "F1 Score: 0.8482\n",
            "Loss: 0.4524\n",
            "Confusion Matrix:\n",
            "[[97  1  1  1]\n",
            " [ 2 93  5  0]\n",
            " [ 3 11 85  1]\n",
            " [16  8  5 41]]\n",
            "Global Round 3/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 3:\n",
            "Accuracy: 0.8838\n",
            "Precision: 0.8882\n",
            "Recall: 0.8838\n",
            "F1 Score: 0.8819\n",
            "Loss: 0.3717\n",
            "Confusion Matrix:\n",
            "[[98  0  1  1]\n",
            " [ 2 95  3  0]\n",
            " [ 0 14 82  4]\n",
            " [ 8  7  3 52]]\n",
            "Global Round 4/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 4:\n",
            "Accuracy: 0.8865\n",
            "Precision: 0.8860\n",
            "Recall: 0.8865\n",
            "F1 Score: 0.8857\n",
            "Loss: 0.3422\n",
            "Confusion Matrix:\n",
            "[[94  0  1  5]\n",
            " [ 2 93  5  0]\n",
            " [ 0 10 85  5]\n",
            " [ 8  2  4 56]]\n",
            "Global Round 5/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 5:\n",
            "Accuracy: 0.8784\n",
            "Precision: 0.8789\n",
            "Recall: 0.8784\n",
            "F1 Score: 0.8767\n",
            "Loss: 0.3380\n",
            "Confusion Matrix:\n",
            "[[96  0  1  3]\n",
            " [ 2 91  6  1]\n",
            " [ 1 10 86  3]\n",
            " [12  2  4 52]]\n",
            "Global Round 6/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 6:\n",
            "Accuracy: 0.9054\n",
            "Precision: 0.9069\n",
            "Recall: 0.9054\n",
            "F1 Score: 0.9040\n",
            "Loss: 0.3252\n",
            "Confusion Matrix:\n",
            "[[97  0  1  2]\n",
            " [ 2 93  5  0]\n",
            " [ 1  4 92  3]\n",
            " [11  1  5 53]]\n",
            "Global Round 7/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 7:\n",
            "Accuracy: 0.9108\n",
            "Precision: 0.9110\n",
            "Recall: 0.9108\n",
            "F1 Score: 0.9099\n",
            "Loss: 0.3010\n",
            "Confusion Matrix:\n",
            "[[96  0  1  3]\n",
            " [ 2 96  2  0]\n",
            " [ 1  4 90  5]\n",
            " [10  2  3 55]]\n",
            "Global Round 8/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 8:\n",
            "Accuracy: 0.9027\n",
            "Precision: 0.9039\n",
            "Recall: 0.9027\n",
            "F1 Score: 0.9021\n",
            "Loss: 0.3050\n",
            "Confusion Matrix:\n",
            "[[94  0  1  5]\n",
            " [ 2 96  2  0]\n",
            " [ 1 11 83  5]\n",
            " [ 5  1  3 61]]\n",
            "Global Round 9/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 9:\n",
            "Accuracy: 0.9216\n",
            "Precision: 0.9221\n",
            "Recall: 0.9216\n",
            "F1 Score: 0.9215\n",
            "Loss: 0.2785\n",
            "Confusion Matrix:\n",
            "[[95  0  1  4]\n",
            " [ 2 94  4  0]\n",
            " [ 1  2 93  4]\n",
            " [ 8  1  2 59]]\n",
            "Global Round 10/10\n",
            "Training on Client 1\n",
            "Dataset size: 1004, Noise multiplier: 1.0000\n",
            "Training on Client 2\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Training on Client 3\n",
            "Dataset size: 1003, Noise multiplier: 1.0000\n",
            "Validation Metrics - Round 10:\n",
            "Accuracy: 0.9162\n",
            "Precision: 0.9163\n",
            "Recall: 0.9162\n",
            "F1 Score: 0.9158\n",
            "Loss: 0.2683\n",
            "Confusion Matrix:\n",
            "[[94  0  1  5]\n",
            " [ 2 97  1  0]\n",
            " [ 2  7 88  3]\n",
            " [ 5  1  4 60]]\n",
            "\n",
            "Final Test Metrics:\n",
            "Accuracy: 0.8757\n",
            "Precision: 0.8793\n",
            "Recall: 0.8757\n",
            "F1 Score: 0.8740\n",
            "Loss: 0.4771\n",
            "Confusion Matrix:\n",
            "[[94  1  2  3]\n",
            " [ 1 91  7  1]\n",
            " [ 0 11 89  0]\n",
            " [18  0  2 50]]\n",
            "Training complete. Model saved.\n"
          ]
        }
      ],
      "source": [
        "# Run the federated learning\n",
        "if __name__ == \"__main__\":\n",
        "    federated_learning()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-tChRmyb7bG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}